# -*- coding: utf-8 -*-
"""Final DL+PCA+ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIWE9cn9ob8CJ7kcDf6ljC8yUwMrHJZo

#Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
from os import listdir
from os.path import join, isfile
import keras

from PIL import Image
from sklearn.preprocessing import LabelBinarizer
from keras.models import Sequential, Model
from tensorflow.keras.layers import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Dense, Conv2D, MaxPool2D , Flatten
from keras.layers import Input, GlobalAveragePooling2D, concatenate, AveragePooling2D
from keras.layers.core import Activation, Flatten, Dropout
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.preprocessing import image
from tensorflow.keras.utils import img_to_array
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
from sklearn.decomposition import PCA

"""#Common Variables & Functions"""

def plots():
  acc = hist.history['accuracy']
  val_acc = hist.history['val_accuracy']
  loss = hist.history['loss']
  val_loss = hist.history['val_loss']
  epochs = range(1, len(acc) + 1)
  #Train and validation accuracy
  plt.plot(epochs, acc, 'b', label='Training accurarcy')
  plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
  plt.title('Training and Validation accurarcy')
  plt.legend()

  plt.figure()
  #Train and validation loss
  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title('Training and Validation loss')
  plt.legend()
  plt.show()

#checkpoint
from keras.callbacks import ModelCheckpoint, EarlyStopping
#early stopping
es = EarlyStopping(monitor= 'val_accuracy', min_delta= 0.01, patience= 5, verbose=1)

#model check point
mc = ModelCheckpoint(filepath="best_model.h5",monitor= 'val_accuracy',min_delta= 0.01,
                     patience= 5, verbose=1 ,save_best_only= True)
cb = [es, mc]

'''from keras import callbacks
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", mode ="min", patience = 5, restore_best_weights = True)'''

"""**Functions for displaying Accuracy Metrics**"""

def display_metrics(a,b):
  conf_matrix = confusion_matrix(a,b)
  FP = conf_matrix[0,1]
  FN = conf_matrix[1,0]
  TP = conf_matrix[0,0]
  TN = conf_matrix[1,1]

  # Sensitivity, hit rate, recall, or true positive rate
  print("Recall/ Sensitivity: ",TP/(TP+FN))
  # Specificity or true negative rate
  print("Specificity: ",TN/(TN+FP))
  # Precision or positive predictive value
  print("Precision: ",TP/(TP+FP))
  # Fall out or false positive rate
  print("False positive rate: ",FP/(FP+TN))
  # Overall accuracy
  print("Accuracy: ",(TP+TN)/(TP+FP+FN+TN) *100)
  print("Classification Report: ")
  print(classification_report(a,b))

import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics

def ROC_Curve(a,b,title,model_name):
  fpr, tpr, thresholds = metrics.roc_curve(a,b)
  roc_auc = metrics.auc(fpr, tpr)
  display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name= model_name)
  display.plot()
  plt.title(title)
  plt.fill_between(fpr, tpr, facecolor='lightgreen', alpha=0.8)
  plt.show()

from sklearn.metrics import ConfusionMatrixDisplay
def confusion_Matrix(a,b) :
  cm = confusion_matrix(a,b,normalize='all')
  cmd = ConfusionMatrixDisplay(cm, display_labels=['False','True'])
  cmd.plot()
  cmd.ax_.set(xlabel='Predicted', ylabel='True')

"""#Load dataset and data Augmentation"""

default_image_size = tuple((32, 32))
directory_root = '/content/drive/MyDrive/Dataset'

# horizontal flip
def hflip(image_dir):
  image = cv2.imread(image_dir)
  image = cv2.flip(image, 0)
  return convert_image_to_array(image)

# vertical flip
def vflip(image_dir):
  image = cv2.imread(image_dir)
  image = cv2.flip(image, 1)
  return convert_image_to_array(image)

# histogram equalization function
def hist(img):
  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)
  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])
  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)
  return hist_equalization_result

#gaussian blurring
from scipy import ndimage
def gaussian(img):
  img = ndimage.gaussian_filter(img, sigma= 5.11)
  return img

# function for rotation
import random
def rotation(img):
  img = cv2.imread(img)
  rows,cols = img.shape[0],img.shape[1]
  randDeg = random.randint(-180, 180)
  matrix = cv2.getRotationMatrix2D((cols/2, rows/2), randDeg, 0.70)
  rotated = cv2.warpAffine(img, matrix, (rows, cols), borderMode=cv2.BORDER_CONSTANT)
  return convert_image_to_array(rotated)

def convert_image_to_array(image_dir):
    try:
        if type(image_dir) is str :
            image = cv2.imread(image_dir)
        else :
            image = image_dir
        if image is not None :
            image = gaussian(image)
            image = hist(image)
            image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
            image = cv2.resize(image, default_image_size)
            return img_to_array(image)
        else :
            return np.array([])
    except Exception as e:
        print(f"Error : {e}")
        return None

image_list, label_list = [], []

try:
    print("[INFO] Loading images ...")
    root_dir = listdir(directory_root)

    for plant_folder in root_dir :
        plant_disease_image_list = listdir(f"{directory_root}/{plant_folder}")

        for image in plant_disease_image_list:
            image_directory = f"{directory_root}/{plant_folder}/{image}"
            image_list.append(convert_image_to_array(image_directory))
            label_list.append(plant_folder)
            image_list.append(hflip(image_directory))
            label_list.append(plant_folder)
            image_list.append(vflip(image_directory))
            label_list.append(plant_folder)
            image_list.append(rotation(image_directory))
            label_list.append(plant_folder)
    print("[INFO] Image loading completed")
except Exception as e:
    print(f"Error : {e}")

"""#Preprocessing"""

label_binarizer = LabelBinarizer()
image_labels = label_binarizer.fit_transform(label_list)
n_classes = len(label_binarizer.classes_)

len(image_list)

np_image_list = np.array(image_list, dtype=np.float16) / 225.0

norm_image_list = (image_list - np.min(image_list)) / (np.max(image_list) - np.min(image_list))

from sklearn.model_selection import train_test_split
print("[INFO] Spliting data to train, test")
x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.33, random_state = 42)

x_train[0]

from sklearn.model_selection import train_test_split
print("[INFO] Spliting data to train, test")
x_train_norm, x_test_norm, y_train_norm, y_test_norm = train_test_split(norm_image_list, image_labels, test_size=0.33, random_state = 42)

x_train_norm[0]

from keras.utils import np_utils
y_test = np_utils.to_categorical(y_test)
y_train=np_utils.to_categorical(y_train)

from keras.utils import np_utils
y_test_norm = np_utils.to_categorical(y_test_norm)
y_train_norm=np_utils.to_categorical(y_train_norm)

print("X_train shape : ",x_train.shape)
print("y_train shape : ",y_train.shape)
print("X_test shape : ",x_test.shape)
print("y_test shape : ",y_test.shape)

print("X_train shape : ",x_train_norm.shape)
print("y_train shape : ",y_train_norm.shape)
print("X_test shape : ",x_test_norm.shape)
print("y_test shape : ",y_test_norm.shape)

"""#define DL models

**VGG16 Model**
"""

def vgg16():
  model = Sequential()
  model.add(Conv2D(input_shape=(32,32,1),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
  model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
  model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
  model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='feature_layer'))
  model.add(Flatten())
  model.add(Dense(units=4096,activation="relu"))
  model.add(Dense(units=4096,activation="relu"))
  model.add(Dense(units=2, activation="softmax"))
  return model

"""**densenet**"""

def conv_layer(conv_x, filters):
    conv_x = BatchNormalization()(conv_x)
    conv_x = Activation('relu')(conv_x)
    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)
    conv_x = Dropout(0.2)(conv_x)
    return conv_x


def dense_block(block_x, filters, growth_rate, layers_in_block):
    for i in range(layers_in_block):
        each_layer = conv_layer(block_x, growth_rate)
        block_x = concatenate([block_x, each_layer], axis=-1)
        filters += growth_rate

    return block_x, filters


def transition_block(trans_x, tran_filters):
    trans_x = BatchNormalization()(trans_x)
    trans_x = Activation('relu')(trans_x)
    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)
    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)

    return trans_x, tran_filters


def dense_net(filters, growth_rate, classes, dense_block_size, layers_in_block):
    input_img = Input(shape=(32, 32, 1))
    x = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)

    dense_x = BatchNormalization()(x)
    dense_x = Activation('relu')(x)

    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='feature_layer')(dense_x)
    for block in range(dense_block_size - 1):
        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)
        dense_x, filters = transition_block(dense_x, filters)

    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)
    dense_x = BatchNormalization()(dense_x)
    dense_x = Activation('relu')(dense_x)
    dense_x = GlobalAveragePooling2D()(dense_x)

    output = Dense(classes, activation='softmax')(dense_x)

    return Model(input_img, output)

def densenet():
  dense_block_size = 1
  layers_in_block = 4

  growth_rate = 12
  classes = 2
  model = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block)
  return model

"""new densenet"""

#%%
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dense
from tensorflow.keras.layers import AvgPool2D, GlobalAveragePooling2D, MaxPool2D
from tensorflow.keras.models import Model
from tensorflow.keras.layers import ReLU, concatenate
import tensorflow.keras.backend as K
# Creating Densenet121
def new_densenet(input_shape=(32,32,1), n_classes=2, filters = 32):

    #batch norm + relu + conv
    def bn_rl_conv(x,filters,kernel=1,strides=1):

        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = Conv2D(filters, kernel, strides=strides,padding = 'same')(x)
        return x

    def dense_block(x, repetition):

        for _ in range(repetition):
            y = bn_rl_conv(x, 4*filters)
            y = bn_rl_conv(y, filters, 3)
            model = concatenate([y,x])
        return model

    def transition_layer(x):

        x = bn_rl_conv(x, K.int_shape(x)[-1] //2 )
        x = AvgPool2D(2, strides = 2, padding = 'same')(x)
        return x

    input = Input (input_shape)
    x = Conv2D(64, 7, strides = 2, padding = 'same')(input)
    x = MaxPool2D(3, strides = 2, padding = 'same')(x)

    for repetition in [6,12,24,16]:

        d = dense_block(x, repetition)
        x = transition_layer(d)
    x = GlobalAveragePooling2D()(d)
    output = Dense(n_classes, activation = 'softmax')(x)

    model = Model(input, output)
    return model

input_shape = 224, 224, 1
n_classes = 2

"""**cnn**"""

def cnn():
  model = Sequential()
  depth=1
  height=32
  width=32
  inputShape = (height, width, depth)
  chanDim = -1
  if K.image_data_format() == "channels_first":
      inputShape = (depth, height, width)
      chanDim = 1
  print(inputShape)
  model.add(Conv2D(32, (3, 3), padding="same",input_shape=inputShape))
  model.add(Activation("relu"))
  model.add(BatchNormalization(axis=chanDim))
  model.add(MaxPooling2D(pool_size=(3, 3)))
  model.add(Dropout(0.25))
  model.add(Conv2D(64, (3, 3), padding="same"))
  model.add(Activation("relu"))
  model.add(BatchNormalization(axis=chanDim))
  model.add(Conv2D(64, (3, 3), padding="same"))
  model.add(Activation("relu"))
  model.add(BatchNormalization(axis=chanDim))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.25))
  model.add(Conv2D(128, (3, 3), padding="same"))
  model.add(Activation("relu"))
  model.add(BatchNormalization(axis=chanDim))
  model.add(Conv2D(128, (3, 3), padding="same"))
  model.add(Activation("relu"))
  model.add(BatchNormalization(axis=chanDim))
  model.add(MaxPooling2D(pool_size=(2, 2),name='feature_layer'))
  model.add(Dropout(0.25))
  model.add(Flatten())
  model.add(Dense(1024))
  model.add(Activation("relu"))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))
  model.add(Dense(n_classes))
  model.add(Activation("softmax"))
  return model

"""resnet"""

import keras
import numpy as np
from tensorflow.keras import initializers
from tensorflow.keras import layers
# initializer =  keras.initializers.glorot_uniform(seed=0)
initializer = initializers.glorot_normal()

"""
Creates Residual Network with 50 layers
"""
def resnet(input_shape=(224, 224, 1), classes=2):
    # Define the input as a tensor with shape input_shape
    X_input = layers.Input(input_shape)

    # Zero-Padding
    X = layers.ZeroPadding2D((3, 3))(X_input)

    # Stage 1
    X = layers.Conv2D(64, (7, 7), strides=(2, 2), name='conv1',
                            kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name='bn_conv1')(X)
    X = layers.Activation('relu')(X)
    X = layers.MaxPooling2D((3, 3), strides=(2, 2))(X)

    # Stage 2
    X = convolutional_block(X, f = 3, filters=[64, 64, 256], stage=2, block='a', s=1)
    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')
    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')

    # Stage 3
    X = convolutional_block(X, f = 3, filters=[128, 128, 512], stage=3, block='a', s=2)
    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')
    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')
    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')

    # Stage 4
    X = convolutional_block(X, f = 3, filters=[256, 256, 1024], stage=4, block='a', s=2)
    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')
    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')
    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')
    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')
    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')

    # Stage 5
    X = convolutional_block(X, f = 3, filters=[512, 512, 2048], stage=5, block='a', s=2)
    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')
    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')

    # AVGPOOL
    X = layers.AveragePooling2D(pool_size=(2, 2),name='feature_layer')(X)

    # output layer
    X = layers.Flatten()(X)
    X = layers.Dense(classes, activation='softmax', name='fc{}'
                            .format(classes), kernel_initializer=initializer)(X)

    # Create model
    model = keras.models.Model(inputs=X_input, outputs=X, name='resnet50')

    return model

"""
Identity Block of ResNet
"""
def identity_block(X, f, filters, stage, block):
    # defining name basis
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'

    # Retrieve Filters
    F1, F2, F3 = filters

    # Save the input value. You'll need this later to add back to the main path.
    X_shortcut = X

    # First component of main path
    X = layers.Conv2D(filters=F1, kernel_size=(1, 1), strides=(1,1), padding='same',
                            name=conv_name_base + '2a', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(X)
    X = layers.Activation('relu')(X)
    X = layers.Dropout(0.5)(X)

    # Second component of main path
    X = layers.Conv2D(filters=F2, kernel_size=(f, f), strides=(1,1), padding='same',
                            name=conv_name_base + '2b', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(X)
    X = layers.Activation('relu')(X)
    X = layers.Dropout(0.5)(X)

    # Third component of main path
    X = layers.Conv2D(filters=F3, kernel_size=(1, 1), strides=(1,1), padding='same',
                            name=conv_name_base + '2c', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(X)

    # Add shortcut value to main path, and pass it through a RELU activation
    X = layers.Add()([X, X_shortcut])
    X = layers.Activation('relu')(X)

    return X

"""
Convolutional Block of ResNet
"""
def convolutional_block(X, f, filters, stage, block, s=2):
    # defining name basis
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'

    # Retrieve Filters
    F1, F2, F3 = filters

    # Save the input value
    X_shortcut = X

    # First component of main path
    X = layers.Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a',
                            padding='same', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2a')(X)
    X = layers.Activation('relu')(X)
    X = layers.Dropout(0.5)(X)

    # Second component of main path
    X = layers.Conv2D(F2, (f, f), strides=(1, 1), name=conv_name_base + '2b',
                            padding='same', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2b')(X)
    X = layers.Activation('relu')(X)
    X = layers.Dropout(0.5)(X)

    # Third component of main path
    X = layers.Conv2D(F3, (1, 1), strides=(1, 1), name=conv_name_base + '2c',
                            padding='same', kernel_initializer=initializer)(X)
    X = layers.BatchNormalization(axis=3, name=bn_name_base + '2c')(X)

    X_shortcut = layers.Conv2D(F3, (1, 1), strides=(s,s), name=conv_name_base + '1',
                                    padding='same', kernel_initializer=initializer)(X_shortcut)
    X_shortcut = layers.BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)

    # Add shortcut value to main path, and pass it through a RELU activation
    X = layers.Add()([X, X_shortcut])
    X = layers.Activation('relu')(X)

    return X

def sum(model):
  model.summary()

def comp(model):
  from keras.optimizers import Adam
  opt = Adam(lr=0.001)
  model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

#checkpoint
from keras.callbacks import ModelCheckpoint, EarlyStopping
#early stopping
es = EarlyStopping(monitor= 'val_accuracy', min_delta= 0.001, patience= 5, verbose=1)

#model check point
mc = ModelCheckpoint(filepath="best_model.h5",monitor= 'val_accuracy',min_delta= 0.01,
                     patience= 5, verbose=1 ,save_best_only= True)
cb = es

def fitmodel(model):
  hist=model.fit(x_train,y_train, epochs=16, batch_size=32, shuffle=True,
                    validation_data=(x_test, y_test),
                #callbacks =cb,
                validation_steps=16)
  return hist

"""# define Performance Metrics"""

def performance(model):
  print("[INFO] Calculating model accuracy")
  scores = model.evaluate(x_test, y_test)
  print(f"Test Accuracy: {scores[1]*100}")
  scores = model.evaluate(x_test, y_test)
  print(f"Train Accuracy: {model.evaluate(x_train,y_train)[1]*100}")

def graph(hist):
  acc = hist.history['accuracy']
  val_acc = hist.history['val_accuracy']
  loss = hist.history['loss']
  val_loss = hist.history['val_loss']
  epochs = range(1, len(acc) + 1)
  #Train and validation accuracy
  plt.plot(epochs, acc, 'b', label='Training accurarcy')
  plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')
  plt.title('Training and Validation accurarcy')
  plt.legend()

  plt.figure()
  #Train and validation loss
  plt.plot(epochs, loss, 'b', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title('Training and Validation loss')
  plt.legend()
  plt.show()

"""#execute dl models"""

model_vgg16=vgg16()
sum(model_vgg16)
comp(model_vgg16)
hist_vgg16=fitmodel(model_vgg16)
performance(model_vgg16)
graph(hist_vgg16)

model_densenet=densenet()
sum(model_densenet)
comp(model_densenet)
hist_densenet=fitmodel(model_densenet)
performance(model_densenet)
graph(hist_densenet)

model_cnn=cnn()
sum(model_cnn)
comp(model_cnn)
hist_cnn=fitmodel(model_cnn)
performance(model_cnn)
graph(hist_cnn)

"""#extract features"""

def extract(model,x_train,x_test,y_train,y_test):
  new_model=Model(inputs=model.input,outputs=model.get_layer('feature_layer').output)

  #Let's obtain the Input Representations
  x_train_n=new_model.predict(x_train)
  x_test_n=new_model.predict(x_test)
  #print(x_train_n[0])
  #Convert back the labels
  y_train_n=[ np.where(r==1)[0][0] for r in y_train ]
  y_test_n=[ np.where(r==1)[0][0] for r in y_test ]

  features=x_train_n.shape[1]*x_train_n.shape[2]*x_train_n.shape[3]
  x_train_new = np.reshape(x_train_n, (-1, features))
  x_test_new = np.reshape(x_test_n, (-1, features))

  x_train=x_train_new
  x_test=x_test_new
  y_train=y_train_n
  y_test=y_test_n

  return x_train,x_test,y_train,y_test

x_train_vgg16,x_test_vgg16,y_train_vgg16,y_test_vgg16=extract(model_vgg16,x_train,x_test,y_train,y_test)

x_train_vgg16_norm,x_test_vgg16_norm,y_train_vgg16_norm,y_test_vgg16_norm=extract(model_vgg16,x_train_norm,x_test_norm,y_train_norm,y_test_norm)

x_train_densenet,x_test_densenet,y_train_densenet,y_test_densenet=extract(model_densenet,x_train,x_test,y_train,y_test)

x_train_cnn,x_test_cnn,y_train_cnn,y_test_cnn=extract(model_cnn,x_train,x_test,y_train,y_test)

"""#apply pca"""

x_train_vgg16.shape

x_train_vgg16

np.count_nonzero(x_train_vgg16==0)

x_train_cnn.shape

x_train_vgg16_norm.shape

x_train_vgg16_norm[0]

from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
x_train_mms_norm = mms.fit_transform(x_train_vgg16_norm)
x_test_mms_norm = mms.transform(x_test_vgg16_norm)

np.seterr(invalid='ignore')

pca_model = PCA(n_components=512)
pca_model.fit(x_train_densenet)

plt.grid()
plt.plot(np.cumsum(pca_model.explained_variance_ratio_ * 100))
plt.xlabel('Number of components')
plt.ylabel('Explained variance')

print(pca_model.explained_variance_)

pca_model = PCA(0.95)
pca_model.fit(x_train_mms_norm)
train_images_reduced = pca_model.fit_transform(x_train_mms_norm)
test_images_reduced = pca_model.transform(x_test_mms_norm)

# verify shape after PCA
print("Train images shape:", train_images_reduced.shape)
print("Test images shape:", test_images_reduced.shape)
# get exact variability retained
print("\nVar retained (%):",
      np.sum(pca_model.explained_variance_ratio_ * 100))

pca_model = PCA(n_components=256)
pca_model.fit(x_train_cnn)
train_images_reduced = pca_model.transform(x_train_cnn)
test_images_reduced = pca_model.transform(x_test_cnn)

# verify shape after PCA
print("Train images shape:", train_images_reduced.shape)
print("Test images shape:", test_images_reduced.shape)
# get exact variability retained
print("\nVar retained (%):",
      np.sum(pca_model.explained_variance_ratio_ * 100))

pca_model = PCA(n_components=256)
pca_model.fit(x_train_densenet)
train_images_reduced = pca_model.transform(x_train_densenet)
test_images_reduced = pca_model.transform(x_test_densenet)

# verify shape after PCA
print("Train images shape:", train_images_reduced.shape)
print("Test images shape:", test_images_reduced.shape)
# get exact variability retained
print("\nVar retained (%):",
      np.sum(pca_model.explained_variance_ratio_ * 100))

def pca(x_train,x_test,model):
  '''f=x_train.shape[1]*x_train.shape[2]*x_train.shape[3]
  pca_model = PCA(n_components=f)
  pca_model.fit(x_train)

  plt.grid()
  plt.plot(np.cumsum(pca_model.explained_variance_ratio_ * 100))
  plt.xlabel('Number of components')
  plt.ylabel('Explained variance')'''

  if model=='densenet':
    components=361
  elif model=='cnn':
    components=256
  elif model=='vgg16':
    components=256
  else:
    components=256

  pca_model = PCA(n_components=components)
  pca_model.fit(x_train)
  train_images_reduced = pca_model.transform(x_train)
  test_images_reduced = pca_model.transform(x_test)

  # verify shape after PCA
  print("Train images shape:", train_images_reduced.shape)
  print("Test images shape:", test_images_reduced.shape)
  # get exact variability retained
  print("\nVar retained (%):",
        np.sum(pca_model.explained_variance_ratio_ * 100))

  x_train=train_images_reduced
  x_test=test_images_reduced

  return x_train,x_test

x_train_red_vgg16,x_test_red_vgg16=pca(x_train_vgg16,x_test_vgg16,vgg16)

x_train_red_densenet,x_test_red_densenet=pca(x_train_densenet,x_test_densenet,densenet)

x_train_red_cnn,x_test_red_cnn=pca(x_train_cnn,x_test_cnn,cnn)

"""#define ML classifiers"""

#K_nearest Classifier
def knn_ml(x_train,x_test,y_train,y_test):
  from sklearn.neighbors import KNeighborsClassifier
  knc=KNeighborsClassifier()
  knc.fit(x_train,y_train)
  knc.score(x_train,y_train)
  knn_y_pred=knc.predict(x_test)
  knn_x_pred=knc.predict(x_train)

  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
  print("Training Data : ")
  display_metrics(y_train,knn_x_pred)
  print("Testing Data : ")
  display_metrics(y_test,knn_y_pred)

  ROC_Curve(y_train,knn_x_pred,"ROC Curve for Train Data","Knn")
  ROC_Curve(y_test,knn_y_pred,"ROC Curve for Test Data","Knn")

  print("Training Data : ")
  print(confusion_matrix(y_train, knn_x_pred))
  confusion_Matrix(y_train,knn_x_pred)

  print("Testing Data : ")
  print(confusion_matrix(y_test,knn_y_pred))
  confusion_Matrix(y_test,knn_y_pred)

#svm
def svm_ml(x_train,x_test,y_train,y_test):
  from sklearn.svm import SVC
  svm = SVC(kernel='linear')
  svm.fit(x_train, y_train)
  svm_y_pred = svm.predict(x_test)
  svm_x_pred = svm.predict(x_train)

  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
  print("Training Data : ")
  display_metrics(y_train,svm_x_pred)
  print("Testing Data : ")
  display_metrics(y_test,svm_y_pred)

  ROC_Curve(y_train,svm_x_pred,"ROC Curve for Train Data","svm")
  ROC_Curve(y_test,svm_y_pred,"ROC Curve for Test Data","svm")

  print("Training Data : ")
  print(confusion_matrix(y_train, svm_x_pred))
  confusion_Matrix(y_train,svm_x_pred)

  print("Testing Data : ")
  print(confusion_matrix(y_test,svm_y_pred))
  confusion_Matrix(y_test,svm_y_pred)

#Naive Bayes
def naivebayes_ml(x_train,x_test,y_train,y_test):
  from sklearn.naive_bayes import GaussianNB
  nb = GaussianNB()
  nb.fit(x_train, y_train)
  nb_y_pred=nb.predict(x_test)
  nb_x_pred=nb.predict(x_train)

  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
  print("Training Data : ")
  display_metrics(y_train,nb_x_pred)
  print("Testing Data : ")
  display_metrics(y_test,nb_y_pred)

  ROC_Curve(y_train,nb_x_pred,"ROC Curve for Train Data","Naive Bayes")
  ROC_Curve(y_test,nb_y_pred,"ROC Curve for Test Data","Naive Bayes")

  print("Training Data : ")
  print(confusion_matrix(y_train, nb_x_pred))
  confusion_Matrix(y_train,nb_x_pred)

  print("Testing Data : ")
  print(confusion_matrix(y_test,nb_y_pred))
  confusion_Matrix(y_test,nb_y_pred)

#logistic regeression
def logreg_ml(x_train,x_test,y_train,y_test):
  from sklearn.linear_model import LogisticRegression
  lr = LogisticRegression(solver='saga')
  lr.fit(x_train, y_train)
  lr_y_pred=lr.predict(x_test)
  lr_x_pred=lr.predict(x_train)

  from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
  print("Training Data : ")
  display_metrics(y_train,lr_x_pred)
  print("Testing Data : ")
  display_metrics(y_test,lr_y_pred)

  ROC_Curve(y_train,lr_x_pred,"ROC Curve for Train Data","Logistic Regression")
  ROC_Curve(y_test,lr_y_pred,"ROC Curve for Test Data","Logistic Regression")

  print("Training Data : ")
  print(confusion_matrix(y_train, lr_x_pred))
  confusion_Matrix(y_train,lr_x_pred)

  print("Testing Data : ")
  print(confusion_matrix(y_test,lr_y_pred))
  confusion_Matrix(y_test,lr_y_pred)

"""#evaluate dl+ml models"""

import warnings
warnings.filterwarnings('always')  # "error", "ignore", "always", "default", "module" or "once"

"""**vgg16**"""

knn_ml(x_train_red_vgg16,x_test_red_vgg16,y_train_vgg16,y_test_vgg16)

svm_ml(x_train_red_vgg16,x_test_red_vgg16,y_train_vgg16,y_test_vgg16)

naivebayes_ml(x_train_red_vgg16,x_test_red_vgg16,y_train_vgg16,y_test_vgg16)

logreg_ml(x_train_red_vgg16,x_test_red_vgg16,y_train_vgg16,y_test_vgg16)

"""**densenet**"""

knn_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)

svm_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)

naivebayes_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)

logreg_ml(x_train_red_densenet,x_test_red_densenet,y_train_densenet,y_test_densenet)

"""**cnn**"""

knn_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)

svm_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)

naivebayes_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)

logreg_ml(x_train_red_cnn,x_test_red_cnn,y_train_cnn,y_test_cnn)